{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow - TensorRT INT8 Inference example from checkpoint\n",
    "In this notebook, we demonstrate the process to create a TF-TensorRT optimized model from a Tensorflow *saved model*.\n",
    "This notebook has been successfully tested in the NVIDIA NGC Tensorflow container `nvcr.io/nvidia/tensorflow:19.04-py3` that can be downloaded from http://ngc.nvidia.com.\n",
    "\n",
    "### Data\n",
    "We use the ImageNet dataset that has been stored in TFrecords format. Google provide an excellent all-in-one script for downloading and preparing the ImageNet dataset at https://github.com/tensorflow/models/blob/master/research/inception/inception/data/download_and_preprocess_imagenet.sh.\n",
    "\n",
    "### Checkpoint\n",
    "We will run this demonstration with a saved model from the Tensorflow Resnet model zoo https://github.com/tensorflow/models/tree/master/official/resnet.\n",
    "\n",
    "To run this notebook, start the NGC TF container providing correct path to ImageNet validation data and a TF saved checkpoint:\n",
    "\n",
    "```bash\n",
    "nvidia-docker run -it -p 8888:8888 -v /path/to/image_net/:/data  -v /path/to/saved_model:/saved_model --name TFTRT nvcr.io/nvidia/tensorflow:19.04-py3\n",
    "```\n",
    "\n",
    "This repository can then be cloned to `/workspace`:\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/vinhngx/tftrt-examples\n",
    "```\n",
    "\n",
    "Then start jupyter notebook within the container with:\n",
    "\n",
    "```bash\n",
    "cd tftrt-examples\n",
    "jupyter notebook --ip 0.0.0.0 --port 8888  --allow-root\n",
    "```\n",
    "\n",
    "Connect to Jupyter notebook web interface from your local host http://localhost:8888. \n",
    "\n",
    "We first install some extra packages and external dependencies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pushd /workspace/nvidia-examples/tensorrt/tftrt/examples/object_detection\n",
    "bash install_dependencies.sh;\n",
    "popd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ii  libnvinfer-dev                         5.0.2-1+cuda10.0                      amd64        TensorRT development libraries and headers\r\n",
      "ii  libnvinfer5                            5.0.2-1+cuda10.0                      amd64        TensorRT runtime libraries\r\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.tensorrt as trt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "\n",
    "#check tensorRT version\n",
    "!dpkg -l | grep nvinfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "We verify that the correct data folder has been mounted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 128 calibration files. \n",
      "/data/validation-00114-of-00128\n",
      "/data/validation-00094-of-00128\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "VALIDATION_DATA_DIR = \"/data\"\n",
    "\n",
    "def get_files(data_dir, filename_pattern):\n",
    "    if data_dir == None:\n",
    "        return []\n",
    "    files = tf.gfile.Glob(os.path.join(data_dir, filename_pattern))\n",
    "    if files == []:\n",
    "        raise ValueError('Can not find any files in {} with '\n",
    "                         'pattern \"{}\"'.format(data_dir, filename_pattern))\n",
    "    return files\n",
    "\n",
    "calibration_files = get_files(VALIDATION_DATA_DIR, 'validation*')\n",
    "print('There are %d calibration files. \\n%s\\n%s\\n...'%(len(calibration_files), calibration_files[0], calibration_files[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF model checkpoint\n",
    "If not already downloaded, we will be downloading and working with a ResNet-50 v1 checkpoint from https://github.com/tensorflow/models/tree/master/official/resnet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file '/saved_model/resnet_v1_50_2016_08_28.tar.gz' exists.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "FILE=/saved_model/resnet_v1_50_2016_08_28.tar.gz\n",
    "if [ -f $FILE ]; then\n",
    "   echo \"The file '$FILE' exists.\"\n",
    "else\n",
    "   echo \"The file '$FILE' in not found. Downloading...\"\n",
    "   wget -P /saved_model/ http://download.tensorflow.org/models/resnet_v1_50_2016_08_28.tar.gz\n",
    "fi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet_v1_50.ckpt\r\n"
     ]
    }
   ],
   "source": [
    "!tar -xzvf /saved_model/resnet_v1_50_2016_08_28.tar.gz -C /saved_model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define some global variables\n",
    "BATCH_SIZE = 8\n",
    "SAVED_MODEL_DIR = \"/saved_model/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "We define a few helper functions to read and preprocess Imagenet data from TFRecord files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deserialize_image_record(record):\n",
    "    feature_map = {\n",
    "        'image/encoded':          tf.FixedLenFeature([ ], tf.string, ''),\n",
    "        'image/class/label':      tf.FixedLenFeature([1], tf.int64,  -1),\n",
    "        'image/class/text':       tf.FixedLenFeature([ ], tf.string, ''),\n",
    "        'image/object/bbox/xmin': tf.VarLenFeature(dtype=tf.float32),\n",
    "        'image/object/bbox/ymin': tf.VarLenFeature(dtype=tf.float32),\n",
    "        'image/object/bbox/xmax': tf.VarLenFeature(dtype=tf.float32),\n",
    "        'image/object/bbox/ymax': tf.VarLenFeature(dtype=tf.float32)\n",
    "    }\n",
    "    with tf.name_scope('deserialize_image_record'):\n",
    "        obj = tf.parse_single_example(record, feature_map)\n",
    "        imgdata = obj['image/encoded']\n",
    "        label   = tf.cast(obj['image/class/label'], tf.int32)\n",
    "        bbox    = tf.stack([obj['image/object/bbox/%s'%x].values\n",
    "                            for x in ['ymin', 'xmin', 'ymax', 'xmax']])\n",
    "        bbox = tf.transpose(tf.expand_dims(bbox, 0), [0,2,1])\n",
    "        text    = obj['image/class/text']\n",
    "        return imgdata, label, bbox, text\n",
    "\n",
    "from preprocessing import vgg_preprocessing\n",
    "def preprocess(record):\n",
    "    # Parse TFRecord\n",
    "    imgdata, label, bbox, text = deserialize_image_record(record)\n",
    "    label -= 1 # Change to 0-based (don't use background class)\n",
    "    try:    image = tf.image.decode_jpeg(imgdata, channels=3, fancy_upscaling=False, dct_method='INTEGER_FAST')\n",
    "    except: image = tf.image.decode_png(imgdata, channels=3)\n",
    "\n",
    "    image = vgg_preprocessing.preprocess_image(image, 224, 224, is_training=False)\n",
    "    return image, label\n",
    "\n",
    "dataset = tf.data.TFRecordDataset(calibration_files)    \n",
    "dataset = dataset.apply(tf.contrib.data.map_and_batch(map_func=preprocess, batch_size=BATCH_SIZE, num_parallel_calls=8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next are two functions to benchmark models, either in a `graph_def` form or a `saved model` form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_frozen_graph(frozen_graph, SAVED_MODEL_DIR=None, dataset=dataset, BATCH_SIZE=8):\n",
    "    with tf.Session(graph=tf.Graph()) as sess:\n",
    "        # prepare dataset iterator\n",
    "        iterator = dataset.make_one_shot_iterator()\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        output_node = tf.import_graph_def(\n",
    "            frozen_graph,\n",
    "            return_elements=['classes'],\n",
    "            name=\"\")\n",
    "        \n",
    "        num_hits = 0\n",
    "        num_predict = 0\n",
    "        print('Warming up for 10 batches...')\n",
    "        for _ in range (10):\n",
    "            image_data = sess.run(next_element)    \n",
    "            img = image_data[0]\n",
    "            label = image_data[1].squeeze()\n",
    "            output = sess.run(['classes:0'], feed_dict={\"input:0\": img})\n",
    "            prediction = output[0]\n",
    "            num_hits += np.sum(prediction == label)\n",
    "            num_predict += len(prediction)\n",
    "            \n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            while True:        \n",
    "                image_data = sess.run(next_element)    \n",
    "                img = image_data[0]\n",
    "                label = image_data[1].squeeze()\n",
    "                output = sess.run(['classes:0'], feed_dict={\"input:0\": img})\n",
    "                prediction = output[0]\n",
    "                num_hits += np.sum(prediction == label)\n",
    "                num_predict += len(prediction)\n",
    "        except tf.errors.OutOfRangeError as e:\n",
    "            pass\n",
    "\n",
    "        print('Accuracy: %.2f%%'%(100*num_hits/num_predict)) \n",
    "        print('Inference speed: %.2f samples/s'%(num_predict/(time.time()-start_time)))\n",
    "        \n",
    "        #Save model for serving\n",
    "        if SAVED_MODEL_DIR:\n",
    "            print('Saving model to %s'%SAVED_MODEL_DIR)\n",
    "            tf.saved_model.simple_save(\n",
    "                session=sess,\n",
    "                export_dir=SAVED_MODEL_DIR,\n",
    "                inputs={\"input\":tf.get_default_graph().get_tensor_by_name(\"input:0\")},\n",
    "                outputs={\"classes\":tf.get_default_graph().get_tensor_by_name(\"classes:0\")},\n",
    "                legacy_init_op=None\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_saved_model(SAVED_MODEL_DIR, dataset=dataset, BATCH_SIZE=8):\n",
    "    with tf.Session(graph=tf.Graph()) as sess:\n",
    "        # Initialize all tfrecord paths\n",
    "        dataset = tf.data.TFRecordDataset(calibration_files)    \n",
    "        dataset = dataset.apply(tf.contrib.data.map_and_batch(map_func=preprocess, batch_size=BATCH_SIZE, num_parallel_calls=8))\n",
    "        iterator = dataset.make_one_shot_iterator()\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        tf.saved_model.loader.load(\n",
    "            sess, [tf.saved_model.tag_constants.SERVING], SAVED_MODEL_DIR)\n",
    "\n",
    "        print('Warming up for 10 batches...')\n",
    "        for _ in range (10):\n",
    "            image_data = sess.run(next_element)    \n",
    "            img = image_data[0]\n",
    "            output = sess.run(['classes:0'], feed_dict={\"input:0\": img})\n",
    "\n",
    "        print('Benchmarking inference engine...')\n",
    "        num_hits = 0\n",
    "        num_predict = 0\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            while True:        \n",
    "                image_data = sess.run(next_element)    \n",
    "                img = image_data[0]\n",
    "                label = image_data[1].squeeze()\n",
    "                output = sess.run(['classes:0'], feed_dict={\"input:0\": img})            \n",
    "                prediction = output[0]\n",
    "                num_hits += np.sum(prediction == label)\n",
    "                num_predict += len(prediction)\n",
    "        except tf.errors.OutOfRangeError as e:\n",
    "            pass\n",
    "\n",
    "        print('Accuracy: %.2f%%'%(100*num_hits/num_predict))\n",
    "        print('Inference speed: %.2f samples/s'%(num_predict/(time.time()-start_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking naitive Tensorflow model\n",
    "\n",
    "We first load and benchmark the Resnet-v1-50 model from TF slim. Note that the checkpoint downloaded from http://download.tensorflow.org/models/resnet_v1_50_2016_08_28.tar.gz doesn't come with any meta data, therefore we will need to employ Slim net factory to get the model definition.\n",
    "\n",
    "The newer checkpoints generated by Tensorflow generally comes with enough meta information to load the network from the achirve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving FP32 model to /saved_model//data/Resnet_FP32/1\n"
     ]
    }
   ],
   "source": [
    "import nets.nets_factory\n",
    "\n",
    "FP32_SAVED_MODEL_DIR = SAVED_MODEL_DIR+\"/data/Resnet_FP32/1\"\n",
    "!rm -rf $FP32_SAVED_MODEL_DIR\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.Session() as sess:\n",
    "        tf_input = tf.placeholder(tf.float32, [None, 224, 224, 3], name='input')\n",
    "        network_fn = nets.nets_factory.get_network_fn('resnet_v1_50', 1000,\n",
    "                is_training=False)\n",
    "        tf_net, tf_end_points = network_fn(tf_input)\n",
    "                \n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, SAVED_MODEL_DIR+\"resnet_v1_50.ckpt\")\n",
    "        \n",
    "        tf_output = tf.identity(tf_net, name='logits')\n",
    "        tf_output_classes = tf.argmax(tf_output, axis=1)        \n",
    "        tf_output_classes = tf.reshape(tf_output_classes, (BATCH_SIZE,), name='classes')\n",
    "        \n",
    "        # freeze graph\n",
    "        frozen_graph = tf.graph_util.convert_variables_to_constants(\n",
    "            sess,\n",
    "            sess.graph_def,\n",
    "            output_node_names=['logits', 'classes']\n",
    "        ) \n",
    "    \n",
    "        #Save model for serving\n",
    "        print('Saving FP32 model to %s'%FP32_SAVED_MODEL_DIR)\n",
    "        tf.saved_model.simple_save(\n",
    "            session=sess,\n",
    "            export_dir=FP32_SAVED_MODEL_DIR,\n",
    "            inputs={\"input\":tf.get_default_graph().get_tensor_by_name(\"input:0\")},\n",
    "            outputs={\"classes\":tf.get_default_graph().get_tensor_by_name(\"classes:0\")},\n",
    "            legacy_init_op=None\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
      "\n",
      "signature_def['serving_default']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "    inputs['input'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 224, 224, 3)\n",
      "        name: input:0\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['classes'] tensor_info:\n",
      "        dtype: DT_INT64\n",
      "        shape: (8)\n",
      "        name: classes:0\n",
      "  Method name is: tensorflow/serving/predict\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --all --dir /saved_model/data/Resnet_FP32/1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up for 10 batches...\n",
      "Benchmarking inference engine...\n",
      "Accuracy: 75.18%\n",
      "Inference speed: 731.36 samples/s\n"
     ]
    }
   ],
   "source": [
    "benchmark_saved_model(FP32_SAVED_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up for 10 batches...\n",
      "Accuracy: 75.18%\n",
      "Inference speed: 741.03 samples/s\n"
     ]
    }
   ],
   "source": [
    "benchmark_frozen_graph(frozen_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking TF-TRT FP32  inference engine\n",
    "\n",
    "Next, we convert the naitive TF FP32 model to TF-TRT FP32 and test the speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240600781"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FP32_SAVED_MODEL_DIR = SAVED_MODEL_DIR+\"/data/Resnet_FP32/1\"\n",
    "\n",
    "TRT_FP32_SAVED_MODEL_DIR = SAVED_MODEL_DIR+\"/data/Resnet_TRT_FP32/1\"\n",
    "!rm -rf $TRT_FP32_SAVED_MODEL_DIR\n",
    "\n",
    "trt_fp32_graph = trt.create_inference_graph(\n",
    "    input_graph_def=None,\n",
    "    outputs=None,\n",
    "    max_batch_size=BATCH_SIZE,\n",
    "    input_saved_model_dir=FP32_SAVED_MODEL_DIR,\n",
    "    output_saved_model_dir=TRT_FP32_SAVED_MODEL_DIR,\n",
    "    precision_mode=\"FP32\")\n",
    "len(trt_fp32_graph.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
      "\n",
      "signature_def['serving_default']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "    inputs['input'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 224, 224, 3)\n",
      "        name: input:0\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['classes'] tensor_info:\n",
      "        dtype: DT_INT64\n",
      "        shape: (8)\n",
      "        name: classes:0\n",
      "  Method name is: tensorflow/serving/predict\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --all --dir $TRT_FP32_SAVED_MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up for 10 batches...\n",
      "Benchmarking inference engine...\n",
      "Accuracy: 75.18%\n",
      "Inference speed: 706.74 samples/s\n"
     ]
    }
   ],
   "source": [
    "benchmark_saved_model(TRT_FP32_SAVED_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up for 10 batches...\n",
      "Accuracy: 75.18%\n",
      "Inference speed: 707.13 samples/s\n"
     ]
    }
   ],
   "source": [
    "benchmark_frozen_graph(trt_fp32_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking TF-TRT FP16 inference engine\n",
    "\n",
    "\n",
    "Next, we convert the naitive TF FP32 model to TF-TRT FP16 and test the speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running against TensorRT version 5.0.2\n",
      "INFO:tensorflow:Restoring parameters from /saved_model//data/Resnet_FP32/1/variables/variables\n",
      "INFO:tensorflow:Froze 267 variables.\n",
      "INFO:tensorflow:Converted 267 variables to const ops.\n",
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: /saved_model//data/Resnet_TRT_FP16/1/saved_model.pb\n"
     ]
    }
   ],
   "source": [
    "TRT_FP16_SAVED_MODEL_DIR = SAVED_MODEL_DIR+\"/data/Resnet_TRT_FP16/1\"\n",
    "!rm -rf $TRT_FP16_SAVED_MODEL_DIR\n",
    "\n",
    "trt_FP16 = trt.create_inference_graph(\n",
    "    input_graph_def=None,\n",
    "    outputs=['classes'],\n",
    "    max_batch_size=BATCH_SIZE,\n",
    "    input_saved_model_dir=FP32_SAVED_MODEL_DIR,\n",
    "    output_saved_model_dir=TRT_FP16_SAVED_MODEL_DIR,\n",
    "    precision_mode=\"FP16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running against TensorRT version 5.0.2\n"
     ]
    }
   ],
   "source": [
    "trt_FP16 = trt.create_inference_graph(\n",
    "    input_graph_def=frozen_graph,\n",
    "    outputs=['classes'],\n",
    "    max_batch_size=BATCH_SIZE,\n",
    "    precision_mode=\"FP16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up for 10 batches...\n",
      "Accuracy: 75.20%\n",
      "Inference speed: 1480.08 samples/s\n",
      "Saving model to /saved_model//data/Resnet_TRT_FP16/1\n"
     ]
    }
   ],
   "source": [
    "!rm -rf $TRT_FP16_SAVED_MODEL_DIR\n",
    "benchmark_frozen_graph(trt_FP16, TRT_FP16_SAVED_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up for 10 batches...\n",
      "Benchmarking inference engine...\n",
      "Accuracy: 75.20%\n",
      "Inference speed: 1485.74 samples/s\n"
     ]
    }
   ],
   "source": [
    "benchmark_saved_model(TRT_FP16_SAVED_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating TFTRT INT8 inference model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating TF-TRT INT8 inference model requires two steps:\n",
    "\n",
    "- Step 1: creating the calibration graph, and run some training data through that graph for INT-8 calibration.\n",
    "\n",
    "- Step 2: converting the calibration graph to the TF-TRT INT8 inference engine\n",
    "\n",
    "### Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrate model on calibration data...\n",
      "Calibration accuracy: 62.50%\n"
     ]
    }
   ],
   "source": [
    "#Now we create the TFTRT INT8 calibration graph\n",
    "trt_int8_calib_graph = trt.create_inference_graph(\n",
    "        input_graph_def=frozen_graph,\n",
    "        outputs=['classes:0'],\n",
    "        max_batch_size=BATCH_SIZE,\n",
    "        max_workspace_size_bytes=1<<32,\n",
    "        precision_mode='INT8')\n",
    "\n",
    "#Then calibrate it with 2 batches of examples\n",
    "N_runs=2\n",
    "with tf.Session(graph=tf.Graph()) as sess:\n",
    "    \n",
    "    output_node = tf.import_graph_def(\n",
    "        trt_int8_calib_graph,\n",
    "        return_elements=['classes'],\n",
    "        name='')\n",
    "    \n",
    "    # Initialize all tfrecord paths\n",
    "    dataset = tf.data.TFRecordDataset(calibration_files)\n",
    "    dataset = dataset.apply(tf.contrib.data.map_and_batch(map_func=preprocess, batch_size=BATCH_SIZE, num_parallel_calls=8))\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    next_element = iterator.get_next()\n",
    "\n",
    "    #print([n.name for n in tf.get_default_graph().as_graph_def().node])\n",
    "    \n",
    "    print('Calibrate model on calibration data...')\n",
    "    num_hits = 0\n",
    "    num_predict = 0\n",
    "    for _ in range(N_runs):\n",
    "            image_data = sess.run(next_element)    \n",
    "            img = image_data[0]\n",
    "            label = image_data[1].squeeze()\n",
    "            prediction = sess.run(output_node[0].outputs[0], feed_dict={\"input:0\": img})            \n",
    "            num_hits += np.sum(prediction == label)\n",
    "            num_predict += len(prediction)\n",
    "    print('Calibration accuracy: %.2f%%'%(100*num_hits/num_predict)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "\n",
    "Now we convert the INT8 calibration graph to the final TF-TRT INT8 inference engine, and benchmark its performance. We will also be saving this engine to a *saved model*, ready to be served elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Int8 inference model from the calibration graph and write to a saved session\n",
    "trt_int8_calibrated_graph=trt.calib_graph_to_infer_graph(trt_int8_calib_graph)\n",
    "output_node = tf.import_graph_def(\n",
    "        trt_int8_calibrated_graph,\n",
    "        return_elements=['classes'],\n",
    "        name='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up for 10 batches...\n",
      "Accuracy: 74.99%\n",
      "Inference speed: 1493.75 samples/s\n",
      "Saving model to /saved_model/INT8\n"
     ]
    }
   ],
   "source": [
    "INT8_SAVED_MODEL_DIR = SAVED_MODEL_DIR + '/data/Resnet_TRT_INT8/1'\n",
    "!rm -rf $INT8_SAVED_MODEL_DIR\n",
    "\n",
    "benchmark_frozen_graph(trt_int8_calibrated_graph, INT8_SAVED_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
      "\n",
      "signature_def['serving_default']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "    inputs['input'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 224, 224, 3)\n",
      "        name: input:0\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['classes'] tensor_info:\n",
      "        dtype: DT_INT64\n",
      "        shape: unknown_rank\n",
      "        name: classes:0\n",
      "  Method name is: tensorflow/serving/predict\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --all --dir $INT8_SAVED_MODEL_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking INT8 saved model\n",
    "\n",
    "Finally we reload and verify the performance of the INT8 saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up for 10 batches...\n",
      "Benchmarking inference engine...\n",
      "Accuracy: 74.99%\n",
      "Inference speed: 1504.92 samples/s\n"
     ]
    }
   ],
   "source": [
    "benchmark_saved_model(INT8_SAVED_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking with synthetic data\n",
    "\n",
    "While benchmarking with real datasets, there are data reading and pre-processing procedures involved. As a result, the GPU is not fully loaded all the time. In this section, we test with synthetic data to test the throughput limit of the GPU.\n",
    "\n",
    "### FP32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Praparing synthetic dataset...\n",
      "Warming up for 10 batches...\n",
      "Inference speed: 715.82 samples/s\n"
     ]
    }
   ],
   "source": [
    "NUM_ITER = 1000\n",
    "dummy_input = np.random.random_sample((BATCH_SIZE,224,224,3))\n",
    "\n",
    "def benchmark_synthetic(frozen_graph):\n",
    "    with tf.Session(graph=tf.Graph()) as sess:\n",
    "        print('Praparing synthetic dataset...')\n",
    "        #with tf.device('/device:GPU:0'):                \n",
    "        inc=tf.constant(dummy_input, dtype=tf.float32)\n",
    "        dataset=tf.data.Dataset.from_tensors(inc)\n",
    "        dataset=dataset.repeat()\n",
    "        iterator=dataset.make_one_shot_iterator()\n",
    "        next_element=iterator.get_next()\n",
    "\n",
    "        output_node = tf.import_graph_def(\n",
    "            frozen_graph,\n",
    "            return_elements=['classes:0'],\n",
    "            input_map={'input:0':next_element},\n",
    "            name=\"\")\n",
    "\n",
    "        print('Warming up for 10 batches...')\n",
    "        for _ in range (10):\n",
    "            sess.run(output_node)\n",
    "            \n",
    "        start_time = time.time()\n",
    "        for _ in range(NUM_ITER):       \n",
    "            sess.run(output_node)\n",
    "        \n",
    "        print('Inference speed: %.2f samples/s'%(NUM_ITER*BATCH_SIZE/(time.time()-start_time)))\n",
    "        \n",
    "benchmark_synthetic(frozen_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-TRT FP32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Praparing synthetic dataset...\n",
      "Warming up for 10 batches...\n",
      "Inference speed: 747.97 samples/s\n"
     ]
    }
   ],
   "source": [
    "benchmark_synthetic(frozen_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Praparing synthetic dataset...\n",
      "Warming up for 10 batches...\n",
      "Inference speed: 1068.86 samples/s\n"
     ]
    }
   ],
   "source": [
    "benchmark_synthetic(trt_FP32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Praparing synthetic dataset...\n",
      "Warming up for 10 batches...\n",
      "Inference speed: 2067.87 samples/s\n"
     ]
    }
   ],
   "source": [
    "benchmark_synthetic(trt_FP16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Praparing synthetic dataset...\n",
      "Warming up for 10 batches...\n",
      "Inference speed: 2084.15 samples/s\n"
     ]
    }
   ],
   "source": [
    "benchmark_synthetic(trt_int8_calibrated_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
